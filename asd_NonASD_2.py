# -*- coding: utf-8 -*-
"""Copy of video_classification for Trim_SSBD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RXDTdfexfLqi1OoNslmR7TUYZzWCG4qK

This has graph.

# Fine-tuning for Video Classification with ðŸ¤— Transformers

This notebook shows how to fine-tune a pre-trained Vision model for Video Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder and fine-tune the model altogether on a labeled dataset.


## Dataset

This notebook uses a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). We'll be using a subset of the dataset to keep the runtime of the tutorial short. The subset was prepared using [this notebook](https://drive.google.com/file/d/1tTScjnyiKrBz84jKe1H_hPGGXffAZuxX/view?usp=sharing) following [this guide](https://www.tensorflow.org/tutorials/load_data/video).

## Model

We'll fine-tune the [VideoMAE model](https://huggingface.co/docs/transformers/model_doc/videomae), which was pre-trained on the [Kinetics 400 dataset](https://www.deepmind.com/open-source/kinetics). You can find the other variants of VideoMAE available on ðŸ¤— Hub [here](https://huggingface.co/models?search=videomae). You can also extend this notebook to use other video models such as [X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip#transformers.XCLIPVisionModel).

**Note** that for models where there's no classification head already available you'll have to manually attach it (randomly initialized). But this is not the case for VideoMAE since we already have a [`VideoMAEForVideoClassification`](https://huggingface.co/docs/transformers/model_doc/xclip#transformers.XCLIPVisionModel) class.

## Data preprocessing

This notebook leverages [TorchVision's](https://pytorch.org/vision/stable/transforms.html) and [PyTorchVideo's](https://pytorchvideo.org/) transforms for applying data preprocessing transformations including data augmentation.



Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly.
"""

model_ckpt = "MCG-NJU/videomae-base" # pre-trained model from which to fine-tune
batch_size = 4 # batch size for training and evaluation

"""Before we start, let's install the `pytorchvideo`, `transformers`, and `evaluate` libraries."""


"""If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.

To be able to share your model with the community, there are a few more steps to follow.

First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your token:
"""

from huggingface_hub import HfApi, HfFolder

# Set the token directly
api = HfApi()
HfFolder.save_token('Put you hugging face token here')  # Saves token in the correct directory that Hugging Face libraries check

# from huggingface_hub import notebook_login
# #  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
# notebook_login()

"""Then you need to install Git-LFS to upload your model checkpoints:"""


"""We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."""

from transformers.utils import send_example_telemetry

send_example_telemetry("video_classification_notebook", framework="pytorch")

"""## Fine-tuning a model on a video classification task

In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) vision models on a Video Classification dataset.

Given a video, the goal is to predict an appropriate class for it, like "archery".

### Loading the dataset

Here we first download the subset archive and un-archive it.
"""

# from huggingface_hub import hf_hub_download


# hf_dataset_identifier = "sayakpaul/ucf101-subset"
# filename = "UCF101_subset.tar.gz"
# file_path = hf_hub_download(
#     repo_id=hf_dataset_identifier, filename=filename, repo_type="dataset"
# )

# !tar xf {file_path}

from tensorflow.keras import layers
from tensorflow import keras

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np
import imageio
import cv2
import os

import gdown
import tarfile

from tensorflow.keras.callbacks import EarlyStopping
from keras import callbacks

from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

import time

from sklearn.metrics import confusion_matrix
import numpy as np

from sklearn.model_selection import train_test_split
import shutil

from tensorflow.keras import layers, Model
from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input

import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from keras import callbacks

# import kerastuner as kt

# from google.colab import drive
# drive.mount('drive')

"""Now, let's investigate what is inside the archive."""

import gdown
import tarfile

# Replace 'file_id' with the actual file ID from your Google Drive link

# !find {dataset_root_path} | head -5

# Load data
import pandas as pd

ArmFlapping_df = pd.read_csv("ArmFlapping.csv")
HeadBanging_df = pd.read_csv("HeadBanging.csv")
Spinning_df = pd.read_csv("Spinning.csv")
TypicalDeveloped_df = pd.read_csv("TypicalDeveloped.csv")

print(f"Total videos for Armflapping: {len(ArmFlapping_df)}")
print(f"Total videos for Headbanging: {len(HeadBanging_df)}")
print(f"Total videos for Spinning: {len(Spinning_df)}")
print(f"Total videos for TypicalDeveloped: {len(TypicalDeveloped_df)}")

# Add folder names to the video_name column for easy access
ArmFlapping_df["video_name"] = "ArmFlapping/" + ArmFlapping_df["video_name"]
HeadBanging_df["video_name"] = "HeadBanging/" + HeadBanging_df["video_name"]
Spinning_df["video_name"] = "Spinning/" + Spinning_df["video_name"]
TypicalDeveloped_df["video_name"] = "TypicalDeveloped/" + TypicalDeveloped_df["video_name"]

ArmFlapping_df.head(5)

HeadBanging_df.head(5)

# Combine dataframes
# combined_df = pd.concat([ArmFlapping_df, HeadBanging_df, Spinning_df])

# combined_df.head

# # Split into train and test sets
# train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)

ArmFlapping_train_df, ArmFlapping_temp_df = train_test_split(ArmFlapping_df, test_size=0.3, random_state=42)  # 70% training, 30% temp
HeadBanging_train_df, HeadBanging_temp_df = train_test_split(HeadBanging_df, test_size=0.3, random_state=42)  # 70% training, 30% temp
Spinning_train_df, Spinning_temp_df = train_test_split(Spinning_df, test_size=0.3, random_state=42)  # 70% training, 30% temp
TypicalDeveloped_train_df, TypicalDeveloped_temp_df = train_test_split(TypicalDeveloped_df,test_size=0.3, random_state=42)

ArmFlapping_val_df, ArmFlapping_test_df = train_test_split(ArmFlapping_temp_df, test_size=0.5, random_state=42)  # Split temp into 50% validation, 50% test
HeadBanging_val_df, HeadBanging_test_df = train_test_split(HeadBanging_temp_df, test_size=0.5, random_state=42)
Spinning_val_df, Spinning_test_df = train_test_split(Spinning_temp_df, test_size=0.5, random_state=42)
TypicalDeveloped_val_df, TypicalDeveloped_test_df = train_test_split(TypicalDeveloped_temp_df, test_size=0.5, random_state=42)

print(f"ArmFlapping Train size: {len(ArmFlapping_train_df)}")
print(f"ArmFlapping Test size: {len(ArmFlapping_test_df)}")
print(f"ArmFlapping Validation size: {len(ArmFlapping_val_df)}")

print(f"HeadBanging Train size: {len(HeadBanging_train_df)}")
print(f"HeadBanging Test size: {len(HeadBanging_test_df)}")
print(f"HeadBanging Validation size: {len(HeadBanging_val_df)}")

print(f"Spinning Train size: {len(Spinning_train_df)}")
print(f"Spinning Test size: {len(Spinning_test_df)}")
print(f"Spinning Validation  size: {len(Spinning_val_df)}")

print(f"TypicalDeveloped Train size: {len(TypicalDeveloped_train_df)}")
print(f"TypicalDeveloped Test size: {len(TypicalDeveloped_test_df)}")
print(f"TypicalDeveloped Validation  size: {len(TypicalDeveloped_val_df)}")

ArmFlapping_train_df

ArmFlapping_test_df

ArmFlapping_val_df

# train_df

# print(f"Total videos for training: {len(train_df)}")
# print(f"Total videos for testing: {len(test_df)}")
# print(f"Total videos for Validation: {len(val_df)}")


dataset_root_path = "VideoMEA"

# Create train and test directories
os.makedirs('VideoMEA/train', exist_ok=True)
os.makedirs('VideoMEA/test', exist_ok=True)
os.makedirs('VideoMEA/val', exist_ok=True)

os.makedirs('VideoMEA/train/ArmFlapping', exist_ok=True)
os.makedirs('VideoMEA/test/ArmFlapping', exist_ok=True)
os.makedirs('VideoMEA/val/ArmFlapping', exist_ok=True)

os.makedirs('VideoMEA/train/Spinning', exist_ok=True)
os.makedirs('VideoMEA/test/Spinning', exist_ok=True)
os.makedirs('VideoMEA/val/Spinning', exist_ok=True)

os.makedirs('VideoMEA/train/HeadBanging', exist_ok=True)
os.makedirs('VideoMEA/test/HeadBanging', exist_ok=True)
os.makedirs('VideoMEA/val/HeadBanging', exist_ok=True)

os.makedirs('VideoMEA/train/TypicalDeveloped', exist_ok=True)
os.makedirs('VideoMEA/test/TypicalDeveloped', exist_ok=True)
os.makedirs('VideoMEA/val/TypicalDeveloped', exist_ok=True)

# def move_files_to_folder(df, source_folder, destination_folder):
#     for filename in df['video_name']:
#         # Extract just the file name, without subdirectories
#         just_filename = os.path.basename(filename)

#         # Construct the full source and destination paths
#         source_path = os.path.join(source_folder, filename)
#         destination_path = os.path.join(destination_folder, just_filename)

#         # Move the file
#         if os.path.exists(source_path):
#             shutil.move(source_path, destination_path)

#         # Update the dataframe
#         df['video_name'].replace(filename, just_filename, inplace=True)

import os
import shutil

def move_files_to_folder(df, source_folder, destination_folder):
    # Check if destination folder exists, if not create it
    if not os.path.exists(destination_folder):
        os.makedirs(destination_folder)

    updated_filenames = []

    for filename in df['video_name']:
        just_filename = os.path.basename(filename)
        source_path = os.path.join(source_folder, filename)
        destination_path = os.path.join(destination_folder, just_filename)

        # Move the file
        if os.path.exists(source_path):
            shutil.move(source_path, destination_path)
            updated_filenames.append(just_filename)
        else:
            print(f"Warning: {source_path} does not exist and will not be moved.")
            updated_filenames.append(filename)  # Keep original if not moved

    # Update the DataFrame outside the loop for better performance
    df['video_name'] = updated_filenames

# # Example of how to call this function:
# move_files_to_folder(ArmFlapping_train_df, 'ArmFlapping', 'VideoMEA/train/ArmFlapping/')
# # Repeat for other datasets and categories

# Move files and update dataframe paths
move_files_to_folder(ArmFlapping_train_df, './', 'VideoMEA/train/ArmFlapping')
move_files_to_folder(ArmFlapping_test_df, './', 'VideoMEA/test/ArmFlapping/')
move_files_to_folder(ArmFlapping_val_df, './', 'VideoMEA/val/ArmFlapping/')

move_files_to_folder(HeadBanging_train_df, './', 'VideoMEA/train/HeadBanging')
move_files_to_folder(HeadBanging_test_df, './', 'VideoMEA/test/HeadBanging')
move_files_to_folder(HeadBanging_val_df, './', 'VideoMEA/val/HeadBanging')

move_files_to_folder(Spinning_train_df, './', 'VideoMEA/train/Spinning')
move_files_to_folder(Spinning_test_df, './', 'VideoMEA/test/Spinning')
move_files_to_folder(Spinning_val_df, './', 'VideoMEA/val/Spinning')

move_files_to_folder(TypicalDeveloped_train_df, './', 'VideoMEA/train/TypicalDeveloped')
move_files_to_folder(TypicalDeveloped_test_df, './', 'VideoMEA/test/TypicalDeveloped')
move_files_to_folder(TypicalDeveloped_val_df, './', 'VideoMEA/val/TypicalDeveloped')

os.makedirs('VideoMEA/train/ArmFlapping/UCF101', exist_ok=True)
os.makedirs('VideoMEA/test/ArmFlapping/UCF101', exist_ok=True)
os.makedirs('VideoMEA/val/ArmFlapping/UCF101', exist_ok=True)

os.makedirs('VideoMEA/train/Spinning/UCF101', exist_ok=True)
os.makedirs('VideoMEA/test/Spinning/UCF101', exist_ok=True)
os.makedirs('VideoMEA/val/Spinning/UCF101', exist_ok=True)

os.makedirs('VideoMEA/train/HeadBanging/UCF101', exist_ok=True)
os.makedirs('VideoMEA/test/HeadBanging/UCF101', exist_ok=True)
os.makedirs('VideoMEA/val/HeadBanging/UCF101', exist_ok=True)

os.makedirs('VideoMEA/train/TypicalDeveloped/UCF101', exist_ok=True)
os.makedirs('VideoMEA/test/TypicalDeveloped/UCF101', exist_ok=True)
os.makedirs('VideoMEA/val/TypicalDeveloped/UCF101', exist_ok=True)

"""Broadly, `dataset_root_path` is organized like so:

```bash
UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    test/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
```

Let's now count the number of total videos we have.
"""

import pathlib

dataset_root_path = pathlib.Path(dataset_root_path)

video_count_train = len(list(dataset_root_path.glob("train/*/*.mp4")))
video_count_val = len(list(dataset_root_path.glob("val/*/*.mp4")))
video_count_test = len(list(dataset_root_path.glob("test/*/*.mp4")))
video_total = video_count_train + video_count_val + video_count_test
print(f"Total videos: {video_total}")

all_video_file_paths = (
    list(dataset_root_path.glob("train/*/*.mp4"))
    + list(dataset_root_path.glob("val/*/*.mp4"))
    + list(dataset_root_path.glob("test/*/*.mp4"))
)
all_video_file_paths[:5]

"""The video paths, when `sorted`, appear like so:

```py
...
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',
'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'
...
 ```

We notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.


 For the validation and evaluation splits, we wouldn't want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that we're using in this tutorial takes this information into account.

Next up, we derive the set of labels we have in the dataset. Let's also create two dictionaries that'll be helpful when initializing the model:

* `label2id`: maps the class names to integers.
* `id2label`: maps the integers to class names.
"""

class_labels = sorted({str(path).split("/")[2] for path in all_video_file_paths})
label2id = {label: i for i, label in enumerate(class_labels)}
id2label = {i: label for label, i in label2id.items()}

print(f"Unique classes: {list(label2id.keys())}.")

"""We've got 10 unique classes. For each class we have 30 videos in the training set.

### Loading the model

In the next cell, we initialize a video classification model where the encoder is initialized with the pre-trained parameters and the classification head is randomly initialized. We also initialize the feature extractor associated to the model. This will come in handy during writing the preprocessing pipeline for our dataset.
"""


from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification


image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
model = VideoMAEForVideoClassification.from_pretrained(
    model_ckpt,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)



"""The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some other (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.

**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics` and it obtains much better performance.

### Constructing the datasets for training

For preprocessing the videos, we'll leverage the [PyTorch Video library](https://pytorchvideo.org/). We start by importing the dependencies we need.
"""

# !pip install torchvision==[SPECIFIC VERSION] torch==[SPECIFIC VERSION]

"""Insert '''
from typing import Any, Callable, Dict, Optional, Tuple

import torch
import torchvision
from torchvision.transforms import functional as F_t
from torchvision.transforms.functional import InterpolationMode
''' code in augmenntations.py code
"""

# cd /usr/local/lib/python3.10/dist-packages/pytorchvideo/transforms

# ls

# cd /usr/local/lib/python3.10/dist-packages/pytorchvideo/transforms/functional.py

# cd /usr/local/lib/python3.10/dist-packages/pytorchvideo

# ls

# cd models/

# ls

# cd


import pytorchvideo.data
from torchvision.transforms import functional as F_t
from pytorchvideo.transforms import (
    ApplyTransformToKey,
    Normalize,
    RandomShortSideScale,
    RemoveKey,
    ShortSideScale,
    UniformTemporalSubsample,
)

from torchvision.transforms import (
    Compose,
    Lambda,
    #RandomCrop,
    #RandomHorizontalFlip,
    Resize,
)

"""For the training dataset transformations, we use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, we keep the transformation chain the same except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorch Video](https://pytorchvideo.org).  

We'll use the `image_processor` associated with the pre-trained model to obtain the following information:

* Image mean and standard deviation with which the video frame pixels will be normalized.
* Spatial resolution to which the video frames will be resized.
"""

import os

mean = image_processor.image_mean
std = image_processor.image_std
if "shortest_edge" in image_processor.size:
    height = width = image_processor.size["shortest_edge"]
else:
    height = image_processor.size["height"]
    width = image_processor.size["width"]
resize_to = (height, width)

num_frames_to_sample = model.config.num_frames
sample_rate = 4
fps = 30
clip_duration = num_frames_to_sample * sample_rate / fps


# Training dataset transformations.
train_transform = Compose(
    [
        ApplyTransformToKey(
            key="video",
            transform=Compose(
                [
                    UniformTemporalSubsample(num_frames_to_sample),
                    Lambda(lambda x: x / 255.0),
                    Normalize(mean, std),
                    RandomShortSideScale(min_size=320, max_size=320),
                    #RandomCrop(resize_to),
                    #RandomHorizontalFlip(p=0.5),
		            Resize(resize_to),
                ]
            ),
        ),
    ]
)

# Training dataset.
train_dataset = pytorchvideo.data.Ucf101(
    data_path=os.path.join(dataset_root_path, "train"),
    clip_sampler=pytorchvideo.data.make_clip_sampler("random", clip_duration),
    decode_audio=False,
    transform=train_transform,
)

# Validation and evaluation datasets' transformations.
val_transform = Compose(
    [
        ApplyTransformToKey(
            key="video",
            transform=Compose(
                [
                    UniformTemporalSubsample(num_frames_to_sample),
                    Lambda(lambda x: x / 255.0),
                    Normalize(mean, std),
                    Resize(resize_to),
                ]
            ),
        ),
    ]
)

# Validation and evaluation datasets.
val_dataset = pytorchvideo.data.Ucf101(
    data_path=os.path.join(dataset_root_path, "val"),
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", clip_duration),
    decode_audio=False,
    transform=val_transform,
)

test_dataset = pytorchvideo.data.Ucf101(
    data_path=os.path.join(dataset_root_path, "test"),
    clip_sampler=pytorchvideo.data.make_clip_sampler("uniform", 3600),
    decode_audio=False,
    transform=val_transform,
)

"""**Note**: The above dataset pipelines are taken from the [official PyTorch Video example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorch Video dataset. So, if you wanted to use a custom dataset not supported off-the-shelf by PyTorch Video, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine."""

# We can access the `num_videos` argument to know the number of videos we have in the
# dataset.
train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos

"""Let's now take a preprocessed video from the dataset and investigate it."""

sample_video = next(iter(train_dataset))
sample_video.keys()

def investigate_video(sample_video):
    """Utility to investigate the keys present in a single video sample."""
    for k in sample_video:
        if k == "video":
            print(k, sample_video["video"].shape)
        else:
            print(k, sample_video[k])

    print(f"Video label: {id2label[sample_video[k]]}")


investigate_video(sample_video)

"""We can also visualize the preprocessed videos for easier debugging."""

# video_tensor = sample_video["video"]
# display_gif(video_tensor)

import imageio
import numpy as np
from IPython.display import Image


def unnormalize_img(img):
    """Un-normalizes the image pixels."""
    img = (img * std) + mean
    img = (img * 255).astype("uint8")
    return img.clip(0, 255)


def create_gif(video_tensor, filename="sample.gif"):
    """Prepares a GIF from a video tensor.

    The video tensor is expected to have the following shape:
    (num_frames, num_channels, height, width).
    """
    frames = []
    for video_frame in video_tensor:
        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())
        frames.append(frame_unnormalized)
    kargs = {"duration": 0.25}
    imageio.mimsave(filename, frames, "GIF", **kargs)
    return filename


def display_gif(video_tensor, gif_name="sample.gif"):
    """Prepares and displays a GIF from a video tensor."""
    video_tensor = video_tensor.permute(1, 0, 2, 3)
    gif_filename = create_gif(video_tensor, gif_name)
    return Image(filename=gif_filename)

"""### Training the model

We'll leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  ðŸ¤— Transformers for training the model. To instantiate a `Trainer`, we will need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on ðŸ¤— Hub.

Most of the training arguments are pretty self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs).
"""

# import os
# os._exit(00)

"""Do Not Delete

Original
"""

# from transformers import TrainingArguments, Trainer

# model_name = model_ckpt.split("/")[-1]
# new_model_name = f"{model_name}-ssbd-trim-yolo"
# num_epochs = 10

# args = TrainingArguments(
#     new_model_name,
#     remove_unused_columns=False,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=5e-5,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_ratio=0.1,
#     logging_steps=10,
#     load_best_model_at_end=True,
#     metric_for_best_model="accuracy",
#     push_to_hub=False,
#     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,

# )

# from transformers import TrainingArguments, Trainer

# model_name = model_ckpt.split("/")[-1]
# new_model_name = f"{model_name}-ssbd-trim-yolo"
# num_epochs = 3

# args = TrainingArguments(
#     new_model_name,
#     remove_unused_columns=False,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=5e-5,
#     per_device_train_batch_size=batch_size,
#     per_device_eval_batch_size=batch_size,
#     warmup_ratio=0.1,
#     logging_steps=10,
#     load_best_model_at_end=True,
#     metric_for_best_model='accuracy',
#     push_to_hub=False,
#     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
#     # max_steps=(len(train_dataset) // batch_size) * num_epochs,  # Adjust as per actual dataset size

# )

from transformers import TrainingArguments, Trainer

model_name = model_ckpt.split("/")[-1]
new_model_name = f"{model_name}-ssbd-trim-yolo"
num_epochs = 100

from math import floor

steps_per_epoch = floor(train_dataset.num_videos / batch_size)
max_steps = steps_per_epoch * num_epochs  # Ensure num_epochs aligns with your desired number of epochs

args = TrainingArguments(
    new_model_name,
    remove_unused_columns=False,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    warmup_ratio=0.1,
    logging_steps=steps_per_epoch,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    push_to_hub=False,
    max_steps=max_steps,
    logging_dir='./logs',
    # max_steps=(len(train_dataset) // batch_size) * num_epochs,  # Adjust as per actual dataset size

)

train_dataset.num_videos

batch_size

(train_dataset.num_videos // batch_size)

steps_per_epoch

max_steps

"""There's no need to define `max_steps` when instantiating `TrainingArguments`. Since the dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__()` method we had to specify `max_steps`.

Next, we need to define a function for how to compute the metrics from the predictions, which will just use the `metric` we'll load now. The only preprocessing we have to do is to take the argmax of our predicted logits:
"""

import evaluate

metric = evaluate.load("accuracy")

from sklearn.metrics import confusion_matrix

def class_wise_accuracy(labels, preds):
    # Create a confusion matrix
    cm = confusion_matrix(labels, preds)
    # Calculate accuracy for each class
    class_accuracy = cm.diagonal() / cm.sum(axis=1)
    return class_accuracy

"""Original"""

# # the compute_metrics function takes a Named Tuple as input:
# # predictions, which are the logits of the model as Numpy arrays,
# # and label_ids, which are the ground-truth labels as Numpy arrays.

# # result = {}
# # accuracies = []
# def compute_metrics(eval_pred):
#     """Computes accuracy on a batch of predictions."""
#     global result
#     global accuracies
#     global accuracy
#     logits, labels = eval_pred # Added
#     predictions = np.argmax(eval_pred.predictions, axis=1)
#     accuracies = class_wise_accuracy(labels, predictions)  # Added
#     accuracy = metric.compute(predictions=predictions, references=eval_pred.label_ids)
#     return accuracy

# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=1)
#     accuracy = metric.compute(predictions=predictions, references=eval_pred.label_ids)
#     return {"eval_accuracy": accuracy}

# result
# accuracies

class_names = ['ASD', 'TD']  # Replace these with your actual class names
class_names_2 = ['ArmFlapping', 'HeadBanging', 'Spinning', 'TypicalDeveloped']

"""It works great"""

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def compute_metrics(eval_pred):
    """Computes various metrics on a batch of predictions."""
    global result
    predictions = np.argmax(eval_pred.predictions, axis=1)
    references = eval_pred.label_ids

    # Create copies of the original predictions and references
    original_predictions = predictions.copy()
    original_references = references.copy()

    for i in range(len(predictions)):
        predictions[i] = 0 if predictions[i] <= 2 else 1
        references[i] = 0 if references[i] <= 2 else 1

    # Calculate metrics for 2 classes
    accuracy = accuracy_score(references, predictions)
    precision = precision_score(references, predictions, average='macro')
    recall = recall_score(references, predictions, average='macro')
    f1 = f1_score(references, predictions, average='macro')
    conf_matrix = confusion_matrix(references, predictions)


    # # Added
    global accuracies
    logits, labels = eval_pred
    #predictions = np.argmax(logits, axis=-1)
    accuracies = class_wise_accuracy(references , predictions)  # Added
    # Assuming you have a function or a way to get the class names
    # class_names = label_processor.get_vocabulary()  # Adjust this to your method of getting class names

    # Compute the classification report
    global report
    report = classification_report(references , predictions, target_names=class_names, output_dict=True)




    #################################################################################
    
    # Calculate metrics for 4 classes
    accuracy_2 = accuracy_score(original_references, original_predictions)
    precision_2 = precision_score(original_references, original_predictions, average='macro')
    recall_2 = recall_score(original_references, original_predictions, average='macro')
    f1_2 = f1_score(original_references, original_predictions, average='macro')
    conf_matrix_2 = confusion_matrix(original_references, original_predictions)


    # # Added
    global accuracies_2
    logits, labels = eval_pred
    #predictions = np.argmax(logits, axis=-1)
    accuracies_2 = class_wise_accuracy(original_references, original_predictions)  # Added

    # Compute the classification report
    global report_2
    report_2 = classification_report(original_references, original_predictions, target_names=class_names_2, output_dict=True)


    #########################################################################


    # Combine all metrics into a dictionary
    result = {
        "accuracy": accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'confusion_matrix': conf_matrix.tolist(),  # Convert to list if necessary for JSON serialization or similar
        'Class_wise_Accuracy':accuracies.tolist(),
	"accuracy_2": accuracy_2,
        'precision_2': precision_2,
        'recall_2': recall_2,
        'f1_score_2': f1_2,
        'confusion_matrix_2': conf_matrix_2.tolist(),  # Convert to list if necessary for JSON serialization or similar
        'Class_wise_Accuracy_2':accuracies_2.tolist(),

    }

    return result

# import numpy as np
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# def compute_metrics(eval_pred, train_pred=None, train_labels=None):
#     """Computes various metrics on a batch of predictions, including training metrics if provided."""
#     global result
#     # Compute evaluation metrics
#     predictions = np.argmax(eval_pred.predictions, axis=1)
#     references = eval_pred.label_ids

      
#     accuracy = accuracy_score(references, predictions)
#     precision = precision_score(references, predictions, average='macro')
#     recall = recall_score(references, predictions, average='macro')
#     f1 = f1_score(references, predictions, average='macro')
#     conf_matrix = confusion_matrix(references, predictions)

#     result = {
#         'accuracy': accuracy,
#         'precision': precision,
#         'recall': recall,
#         'f1_score': f1,
#         'confusion_matrix': conf_matrix.tolist()
#     }

#     # Compute training metrics if training data is provided
#     if train_pred is not None and train_labels is not None:
#         train_predictions = np.argmax(train_pred, axis=1)
#         train_accuracy = accuracy_score(train_labels, train_predictions)
#         result['train_accuracy'] = train_accuracy

#     return result

"""**A note on evaluation**:

In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.

We also define a `collate_fn`, which will be used to batch examples together.
Each batch consists of 2 keys, namely `pixel_values` and `labels`.

Original
"""

import torch


def collate_fn(examples):
    """The collation function to be used by `Trainer` to prepare data batches."""
    # permute to (num_frames, num_channels, height, width)
    pixel_values = torch.stack(
        [example["video"].permute(1, 0, 2, 3) for example in examples]
    )
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}

"""Then we just need to pass all of this along with our datasets to the `Trainer`:

Original
"""

# trainer = Trainer(
#     model,
#     args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     tokenizer=image_processor,
#     compute_metrics=compute_metrics,
#     data_collator=collate_fn,
# )

import os

# Set the environment variable for CUDA memory allocation
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import torch
torch.cuda.empty_cache()

from transformers import Trainer, EarlyStoppingCallback

from transformers import Trainer, TrainerCallback, TrainingArguments
from copy import deepcopy
import torch
from transformers import TrainerCallback

# class EarlyStoppingCallback(TrainerCallback):
#     def __init__(self, num_steps=10):
#         self.num_steps = num_steps

#     def on_step_end(self, args, state, control, **kwargs):
#         if state.global_step >= self.num_steps:
#             control.should_training_stop = True
#         return control

from transformers import TrainerCallback

# class EarlyStoppingCallback(TrainerCallback):
#     def __init__(self, monitor='eval_loss', patience=3, verbose=True, delta=0.0):
#         self.monitor = monitor
#         self.patience = patience
#         self.verbose = verbose
#         self.delta = delta
#         self.best_score = None
#         self.no_improve_epochs = 0

#     def on_epoch_end(self, args, state, control, **kwargs):
#         metrics = kwargs.get('metrics', {})
#         current_score = metrics.get(self.monitor)

#         if current_score is None:
#             if self.verbose:
#                 print(f"Warning: The monitor `{self.monitor}` is not found in the metrics.")
#             return control

#         if self.best_score is None or (self.monitor.endswith('loss') and current_score < self.best_score - self.delta) \
#             or (not self.monitor.endswith('loss') and current_score > self.best_score + self.delta):
#             self.best_score = current_score
#             self.no_improve_epochs = 0
#         else:
#             self.no_improve_epochs += 1
#             if self.no_improve_epochs >= self.patience:
#                 if self.verbose:
#                     print(f"EarlyStopping: Stopping training as `{self.monitor}` has not improved in the last {self.patience} epochs.")
#                 control.should_training_stop = True

#         return control


class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -> None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        if control.should_evaluate:
            control_copy = deepcopy(control)
            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix="train")
            return control_copy

trainer = Trainer(
    model,
    args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=image_processor,
    compute_metrics=compute_metrics,
    data_collator=collate_fn,
)
trainer.add_callback(CustomCallback(trainer))

# #calling trainer
# train = trainer.train()

os.makedirs("All_output")

"""Calling the trainer"""

import time
import torch

# Assuming 'trainer' is an instance of a Trainer class or similar setup
start_time = time.time()  # Record the start time

# Start the training process
train = trainer.train()

end_time = time.time()  # Record the end time
training_duration = end_time - start_time  # Calculate the training duration

# Optionally, print the training time
print(f"Training completed in {training_duration:.2f} seconds.")

# Save the training time to a file
with open('./All_output/training_time.txt', 'w') as file:
    file.write(f"Training completed in {training_duration:.2f} seconds.")

print("Training time has been saved to 'training_time.txt'")

"""You might wonder why we pass along the `image_processor` as a tokenizer when we already preprocessed our data. This is only to make sure the feature extractor configuration file (stored as JSON) will also be uploaded to the repo on the hub.

Now we can finetune our model by calling the `train` method:
"""

import os

# Set the environment variable for CUDA memory allocation
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import torch
torch.cuda.empty_cache()

# cd /usr/local/lib/python3.10/dist-packages/transformers/

# train_results = trainer.train()

print(train.metrics)

import logging

logging.basicConfig(level=logging.INFO)

# Access the log history directly from the trainer state
log_history = trainer.state.log_history
print("Complete Log History:")
for entry in log_history:
    print(entry)


# Write the log history to a text file
with open('./All_output/VideoMAE_log_history.txt', 'w') as file:
    file.write("Complete Log History:\n")
    for entry in log_history:
        file.write(str(entry) + '\n')

print("Log history saved to log_history.txt")

train_accuracy = [entry['train_accuracy'] for entry in log_history if 'train_accuracy' in entry]
print(train_accuracy)

eval_acc = [entry['eval_accuracy'] for entry in log_history if 'eval_accuracy' in entry]
print(eval_acc)

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

# Assuming 'loss' and 'eval_loss' are recorded in the log history
training_losses = [entry['train_loss'] for entry in log_history if 'train_loss' in entry]
eval_losses = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]

training_losses = training_losses[:-1]

plt.figure(figsize=(10, 5))
plt.plot(training_losses, label='Training Loss')
plt.plot(eval_losses, label='Validation Loss')

# Setting the x-axis to show only integer labels
ax = plt.gca()  # Get the current Axes instance
ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set major ticks to integers

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss vs Epoch plot of VideoMAE model on modified SSBD dataset')
plt.legend()
plt.savefig('./All_output/Loss_over_time.png')
plt.show()

import matplotlib.pyplot as plt

# Assuming 'loss' and 'eval_loss' are recorded in the log history
training_losses = [entry['train_accuracy'] for entry in log_history if 'train_accuracy' in entry]
eval_losses = [entry['eval_accuracy'] for entry in log_history if 'eval_accuracy' in entry]

plt.figure(figsize=(10, 5))
plt.plot(training_losses, label='Training Accuracy')
plt.plot(eval_losses, label='Validation Accuracy')

# Setting the x-axis to show only integer labels
ax = plt.gca()  # Get the current Axes instance
ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Set major ticks to integers

plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Epoch plot of VideoMAE model on modified SSBD dataset')
plt.legend()
plt.savefig('./All_output/Accuracy_over_time.png')
plt.show()

print(result)

# print(result[0])

"""We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"""

# accur = []
# def compute_metrics(eval_pred):
#     global accur
#     logits, labels = eval_pred
#     predictions = np.argmax(logits, axis=-1)
#     accur = class_wise_accuracy(labels, predictions)
#     return {"class_0_accuracy": accuracies[0], "class_1_accuracy": accuracies[1], "class_2_accuracy": accuracies[2]}  # Continue for all classes

# trainer = Trainer(
#     model,                         # the instantiated Transformers model to be trained
#     args,                  # training arguments, defined above
#     train_dataset=train_dataset,         # training dataset
#     eval_dataset=val_dataset,          # evaluation dataset
#     compute_metrics=compute_metrics,     # the callback that computes metrics of interest
#     tokenizer=image_processor,
#     data_collator=collate_fn,
# )
# trainer.add_callback(CustomCallback(trainer))

test_result = trainer.evaluate(test_dataset)

"""Class-wise Accuracy"""

# print("Class-wise Accuracy:")
# for class_name, accuracy in zip(class_names, accuracies):
#     print(f"{class_name}: {accuracy:.2f}")

with open('./All_output/class_wise_accuracy.txt', 'w') as file:
    file.write("Class-wise Accuracy:\n")
    print("Class-wise Accuracy:")

    # Iterate over each class and its corresponding accuracy
    for class_name, accuracy in zip(class_names, accuracies):
        accuracy_message = f"{class_name}: {accuracy:.2f}"

        # Print to console
        print(accuracy_message)

        # Write to file
        file.write(accuracy_message + '\n')

print("Class-wise accuracy has been saved to 'class_wise_accuracy.txt'")

########################
with open('./All_output/class_wise_accuracy_2.txt', 'w') as file:
    file.write("Class-wise Accuracy:\n")
    print("Class-wise Accuracy:")

    # Iterate over each class and its corresponding accuracy
    for class_name, accuracy in zip(class_names_2, accuracies_2):
        accuracy_message = f"{class_name}: {accuracy:.2f}"

        # Print to console
        print(accuracy_message)

        # Write to file
        file.write(accuracy_message + '\n')

print("Class-wise accuracy has been saved to 'class_wise_accuracy_2.txt'")



########################
print("Test Results")

test_result

# Write the dictionary to a text file
with open('./All_output/Test_Result.txt', 'w') as file:
    for key, value in test_result.items():
        file.write(f"{key}: {value}\n")

print("Result dictionary saved to result.txt")

print("Classification report")

report

# Write the dictionary to a text file
with open('./All_output/Classification report.txt', 'w') as file:
    for key, value in report.items():
        file.write(f"{key}: {value}\n")

print("Result dictionary saved to result.txt")

cm = test_result['eval_confusion_matrix']

###############################################

print("Classification report 2")

report_2

# Write the dictionary to a text file
with open('./All_output/Classification report_2.txt', 'w') as file:
    for key, value in report_2.items():
        file.write(f"{key}: {value}\n")

print("Result dictionary saved to result.txt")

cm = test_result['eval_confusion_matrix']


###############################################

cm

"""Model Summary"""


# from torchinfo import summary
from transformers import AutoModel

# summary(model, input_size=(1, 512))

print(model)

"""Counting Parameters"""

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"Total Parameters: {total_params}")
print(f"Trainable Parameters: {trainable_params}")

import torch
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# cm = np.array(test_result['eval_confusion_matrix'])

# Initialize the display object for the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=np.array(test_result['eval_confusion_matrix']), display_labels=class_names)

# Use Matplotlib to display the confusion matrix
fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(cmap=plt.cm.Reds, ax=ax)
disp.ax_.set_title("CM for modified SSBD")

plt.savefig('./All_output/confusion_matrix.png', format='png')

plt.show()

# accuracies

# print(type(train_results))  # Should confirm it's a tuple
# print(len(train_results))  # Check how many elements are in the tuple
# print(train_results)       # Look at the contents of the tuple

# # Access the log history directly from the trainer state
# log_history = trainer.state.log_history
# print("Complete Log History:")
# for entry in log_history:
#     print(entry)

# # accuracy_per_class = trainer.state.log_history["eval_accuracy_per_class"]

# eval_accuracy_per_class = [entry['eval_accuracy_per_class'] for entry in log_history if 'eval_accuracy_per_class' in entry]
# print(eval_accuracy_per_class)

# for class_name, accuracy in accuracy_per_class.items():
#     print(f"Accuracy for {class_name}: {accuracy}")

"""**Confusion Matrix**"""

# import torch
# import numpy as np
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import ConfusionMatrixDisplay
# import matplotlib.pyplot as plt

# # Function to collect predictions
# def get_predictions(trainer, dataset):
#     trainer.model.eval()  # Set the model to evaluation mode
#     predictions = []
#     labels = []

#     with torch.no_grad():  # Turn off gradients, since they aren't needed for evaluation
#         for batch in trainer.get_eval_dataloader(dataset):
#             batch = {k: v.to(trainer.args.device) for k, v in batch.items()}  # Move batch to device
#             outputs = trainer.model(**batch)
#             logits = outputs.logits
#             preds = torch.argmax(logits, dim=-1)

#             predictions.extend(preds.cpu().numpy())  # Store predictions
#             labels.extend(batch['labels'].cpu().numpy())  # Store true labels

#     return predictions, labels

# # Collect all predictions and true labels from the test dataset
# predictions, true_labels = get_predictions(trainer, test_dataset)

# # Compute the confusion matrix
# cm = confusion_matrix(true_labels, predictions, labels=list(id2label.keys()))

# # Initialize the display object for the confusion matrix
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[id2label[i] for i in id2label.keys()])

# # Use Matplotlib to display the confusion matrix
# fig, ax = plt.subplots(figsize=(10, 10))
# disp.plot(cmap=plt.cm.Reds, ax=ax)
# disp.ax_.set_title("Confusion Matrix")

# plt.show()

"""**End Of Confusion matrix**"""

# train_output = train_results  # Assuming train_results is the TrainOutput object

# # Extracting total epochs and training loss
# total_epochs = train_output.metrics['epoch']
# training_loss = train_output.training_loss

# import matplotlib.pyplot as plt

# # Create a figure for plotting
# plt.figure(figsize=(6, 4))

# # Plot training loss
# plt.plot(total_epochs, training_loss, 'bo', label='Training Loss')  # 'bo' for blue dot
# plt.title('Training Loss at Last Epoch')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()
# plt.show()

# from google.colab import drive
# drive.mount('/content/drive')

# trainer.save_model()
# test_results = trainer.evaluate(test_dataset)
# trainer.log_metrics("test", test_results)
# trainer.save_metrics("test", test_results)
# trainer.save_state()

# training_loss = []
# eval_loss = []
# eval_accuracy = []

# for entry in trainer.state.log_history:
#     if 'loss' in entry and 'eval_loss' not in entry:  # Filter for training loss
#         training_loss.append(entry['loss'])
#     if 'eval_loss' in entry:  # Filter for evaluation loss
#         eval_loss.append(entry['eval_loss'])
#     if 'eval_accuracy' in entry:  # Filter for evaluation accuracy (if your metric)
#         eval_accuracy.append(entry['eval_accuracy'])

# import matplotlib.pyplot as plt

# epochs = range(1, len(training_loss) + 1)

# plt.figure(figsize=(10, 5))
# plt.plot(epochs, training_loss, label='Training Loss')
# plt.plot(epochs, eval_loss, label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()
# plt.show()

# training_loss = [entry['loss'] for entry in trainer.state.log_history if 'loss' in entry and 'eval_loss' not in entry]
# eval_loss = [entry['eval_loss'] for entry in trainer.state.log_history if 'eval_loss' in entry]

# import matplotlib.pyplot as plt

# # Create epoch markers based on actual log entries
# training_epochs = range(1, len(training_loss) + 1)
# eval_epochs = range(1, len(eval_loss) + 1)

# plt.figure(figsize=(10, 5))
# plt.plot(training_epochs, training_loss, 'b-', label='Training Loss')
# if eval_loss:  # Check if there are any evaluation losses logged
#     plt.plot(eval_epochs, eval_loss, 'r-', label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.legend()
# plt.show()

# print("Number of training loss entries:", len(training_loss))
# print("Number of evaluation loss entries:", len(eval_loss))

# import matplotlib.pyplot as plt
# import numpy as np

# def plot_history(trainer):
#     """
#     Plots training and validation loss and accuracy from the logs generated by Hugging Face's Trainer.

#     Args:
#       trainer: An instance of transformers.Trainer that has completed at least one training epoch.
#     """
#     training_loss = [entry['loss'] for entry in trainer.state.log_history if 'loss' in entry and 'eval_loss' not in entry]
#     eval_loss = [entry['eval_loss'] for entry in trainer.state.log_history if 'eval_loss' in entry]
#     training_accuracy = [entry['eval_accuracy'] for entry in trainer.state.log_history if 'eval_accuracy' in entry]
#     eval_accuracy = [entry['eval_accuracy'] for entry in trainer.state.log_history if 'eval_accuracy' in entry]

#     epochs = range(1, len(training_loss) + 1)
#     eval_epochs = range(1, len(eval_loss) + 1)
#     accuracy_epochs = range(1, len(training_accuracy) + 1)

#     fig, (ax1, ax2) = plt.subplots(2, figsize=(18.5, 10.5))

#     # Plotting loss
#     ax1.set_title('Loss')
#     if training_loss:
#         ax1.plot(epochs, training_loss, label='Train Loss')
#     if eval_loss:
#         ax1.plot(eval_epochs, eval_loss, label='Validation Loss')
#     ax1.set_ylabel('Loss')
#     ax1.set_xlabel('Epoch')
#     ax1.legend()

#     # Plotting accuracy
#     ax2.set_title('Accuracy')
#     if training_accuracy:
#         ax2.plot(accuracy_epochs, training_accuracy, label='Train Accuracy')
#     if eval_accuracy:
#         ax2.plot(accuracy_epochs, eval_accuracy, label='Validation Accuracy')
#     ax2.set_ylabel('Accuracy')
#     ax2.set_ylim([0, 1])
#     ax2.set_xlabel('Epoch')
#     ax2.legend()

#     plt.show()

# # Usage
# plot_history(trainer)

# import matplotlib.pyplot as plt
# import numpy as np

# def plot_history(trainer):
#     """
#     Plots validation loss and accuracy from the logs generated by Hugging Face's Trainer.

#     Args:
#       trainer: An instance of transformers.Trainer that has completed at least one training epoch.
#     """
#     eval_loss = [entry['eval_loss'] for entry in trainer.state.log_history if 'eval_loss' in entry]
#     eval_accuracy = [entry['eval_accuracy'] for entry in trainer.state.log_history if 'eval_accuracy' in entry]

#     eval_epochs_loss = range(1, len(eval_loss) + 1)
#     eval_epochs_accuracy = range(1, len(eval_accuracy) + 1)

#     fig, (ax1, ax2) = plt.subplots(2, figsize=(18.5, 10.5))

#     # Plotting validation loss
#     ax1.set_title('Validation Loss')
#     ax1.plot(eval_epochs_loss, eval_loss, 'r-', label='Validation Loss')
#     ax1.set_ylabel('Loss')
#     ax1.set_xlabel('Epoch')
#     ax1.legend()

#     # Plotting validation accuracy
#     ax2.set_title('Validation Accuracy')
#     ax2.plot(eval_epochs_accuracy, eval_accuracy, 'g-', label='Validation Accuracy')
#     ax2.set_ylabel('Accuracy')
#     ax2.set_ylim([0, 1])
#     ax2.set_xlabel('Epoch')
#     ax2.legend()

#     plt.show()

# # Usage
# plot_history(trainer)

# import csv
# import os

# def save_logs_to_csv(trainer, filename="training_logs.csv", path="."):
#     """
#     Saves the training logs to a CSV file at a specified path.

#     Args:
#       trainer: An instance of transformers.Trainer
#       filename: The filename for the saved CSV
#       path: Directory path where the CSV file will be saved
#     """
#     full_path = os.path.join(path, filename)
#     fieldnames = ['step', 'loss', 'eval_loss', 'eval_accuracy']
#     with open(full_path, mode='w', newline='') as file:
#         writer = csv.DictWriter(file, fieldnames=fieldnames)
#         writer.writeheader()
#         for entry in trainer.state.log_history:
#             if 'loss' in entry or 'eval_loss' in entry or 'eval_accuracy' in entry:
#                 row = {field: entry.get(field, None) for field in fieldnames}
#                 writer.writerow(row)

# # Example usage
# save_logs_to_csv(trainer, path="/content/drive/MyDrive/Yolo_Augmentation_Data/Trim_SSBD_Yolo7/Augmentation")

# import torch

# def get_predictions(trainer, dataloader):
#     trainer.model.eval()  # Set model to evaluation mode
#     predictions = []
#     actuals = []

#     for batch in dataloader:
#         # Filter and move only tensor objects to the device
#         inputs = {k: v.to(trainer.args.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
#         labels = batch['labels'].to(trainer.args.device)
#         with torch.no_grad():
#             outputs = trainer.model(**inputs)
#         logits = outputs.logits
#         preds = torch.argmax(logits, dim=-1)
#         predictions.extend(preds.cpu().numpy())
#         actuals.extend(labels.cpu().numpy())

#     return actuals, predictions

# import matplotlib.pyplot as plt
# import seaborn as sns
# import torch

# def plot_confusion_matrix(actual, predicted, labels, ds_type):
#     cm = torch.confusion_matrix(torch.tensor(actual), torch.tensor(predicted))
#     plt.figure(figsize=(12, 12))
#     ax = sns.heatmap(cm, annot=True, fmt='g')
#     sns.set(font_scale=1.4)  # for label size
#     ax.set_title('Confusion Matrix of Action Recognition for ' + ds_type)
#     ax.set_xlabel('Predicted Action')
#     ax.set_ylabel('Actual Action')
#     ax.set_xticklabels(labels, rotation=90)
#     ax.set_yticklabels(labels, rotation=0)
#     plt.show()

# from torch.utils.data import DataLoader


# from transformers import Trainer, TrainingArguments

# # Assuming you have already loaded your dataset
# # For example, a Hugging Face dataset:
# # dataset = load_dataset('some-dataset')
# # or you might have a custom dataset object already defined and loaded.


# val_dataset = DataLoader(dataset, batch_size=16, shuffle=True)

# # Use the DataLoader
# actual_labels, predicted_labels = get_predictions(trainer, val_dataset)
# plot_confusion_matrix(actual_labels, predicted_labels, list(id2label.values()), "Validation")

# for batch in dataloader:
#     print(batch)  # Print the batch to see all its contents and types
#     inputs = {k: v.to(trainer.args.device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
#     labels = batch['labels'].to(trainer.args.device)
#     ...

# # Ensure that your DataLoader is properly configured to return the expected format
# # For instance, it should yield batches that include tensor data and labels.
# actual_labels, predicted_labels = get_predictions(trainer, val_dataset)
# plot_confusion_matrix(actual_labels, predicted_labels, list(id2label.values()), "Validation")

# from google.colab import drive
# drive.mount('/content/drive')

save_path = './'

"""**Saving Model**"""

torch.save(model.state_dict(), os.path.join(save_path, 'All_output/VideoMEA_saved_model_Trim_SSBD_Aug.pth'))

"""You can now upload the result of the training to the Hub, just execute this instruction (note that the Trainer will automatically create a model card as well as Tensorboard logs - see the "Training metrics" tab - amazing isn't it?):"""

# trainer.push_to_hub()

"""Now that our model is trained, let's use it to run inference on a video from `test_dataset`.

## Inference

Let's load the trained model checkpoint and fetch a video from `test_dataset`.
"""

# trained_model = VideoMAEForVideoClassification.from_pretrained(new_model_name)

# sample_test_video = next(iter(test_dataset))
# investigate_video(sample_test_video)

"""We then prepare the video as a `torch.Tensor` and run inference."""

# def run_inference(model, video):
#     """Utility to run inference given a model and test video.

#     The video is assumed to be preprocessed already.
#     """
#     # (num_frames, num_channels, height, width)
#     perumuted_sample_test_video = video.permute(1, 0, 2, 3)

#     inputs = {
#         "pixel_values": perumuted_sample_test_video.unsqueeze(0),
#         "labels": torch.tensor(
#             [sample_test_video["label"]]
#         ),  # this can be skipped if you don't have labels available.
#     }
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     inputs = {k: v.to(device) for k, v in inputs.items()}
#     model = model.to(device)

#     # forward pass
#     with torch.no_grad():
#         outputs = model(**inputs)
#         logits = outputs.logits

#     return logits

# logits = run_inference(trained_model, sample_test_video["video"])

"""We can now check if the model got the prediction right."""

# display_gif(sample_test_video["video"])

# predicted_class_idx = logits.argmax(-1).item()
# print("Predicted class:", model.config.id2label[predicted_class_idx])

"""And it looks like it got it right!

You can also use this model to bring in your own videos. Check out [this Space](https://huggingface.co/spaces/sayakpaul/video-classification-ucf101-subset) to know more. The Space will also show you how to run inference for a single video file.

<br><div align=center>
    <img src="https://i.ibb.co/7nW4Rkn/sample-results.gif" width=700/>
</div>

## Next steps

Now that you've learned to train a well-performing video classification model on a custom dataset here is some homework for you:

* Increase the dataset size: include more classes and more samples per class.
* Try out different hyperparameters to study how the model converges.
* Analyze the classes for which the model fails to perform well.
* Try out a different video encoder.

Don't forget to share your models with the community =)
"""
